{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.  \n",
    "\n",
    "*(see HOML 10-4)*\n",
    "\n",
    "Why was the sigmoid activation function a key ingredient in training the first MLPs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic sigmoid activation function is smooth at all points so it's derivative is always non zero which allows the gradient descent method to find smaller values of theta after each iteration.",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. \n",
    "\n",
    "*(See HOML 10-5)* \n",
    "\n",
    "Name three popular activation functions. Can you draw them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Rectified Linear Activation (ReLU)
2. Logistic (Sigmoid)
3. Hyperbolic Tangent (Tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. \n",
    "\n",
    "*(See HOML 10-6)*\n",
    "\n",
    "Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a.) What is the shape of the input matrix $X$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(your answer in this markdown cell)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b.) What are the shapes of the hidden layer’s weight matrix $W_h$ and bias vector $b_h$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(your answer in this markdown cell)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c.) What are the shapes of the output layer’s weight matrix $W_o$ and bias vector $b_o$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(your answer in this markdown cell)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d.) What is the shape of the network’s output matrix $Y$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(your answer in this markdown cell)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e.) Write the equation that computes the network’s output matrix $Y$ as a function of $X$, $W_h$, $b_h$, $W_o$, and $b_o$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(your answer in this markdown cell)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.\n",
    "\n",
    "*(See HOML 10-9)*\n",
    "\n",
    "Can you list all the hyperparameters you can tweak in a basic MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(your answer in this markdown cell)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.\n",
    "Do all the activities in HOML 10-1:\n",
    "\n",
    "The [TensorFlow playground](https://playground.tensorflow.org/) is a handy neural network simulator built by the TensorFlow team. In this exercise, you will train several binary classifiers in just a few clicks, and tweak the model’s architecture and its hyperparameters to gain some intuition on how neural networks work and what their hyperparameters do. \n",
    "\n",
    "There isn't anything to submit for any of the parts except for part (g.).  \n",
    "\n",
    "**Take some time to explore the following:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a.) **The patterns learned by a neural net.** Try training the default neural network by clicking the Run button (top left). Notice how it quickly finds a good solution for the classification task. The neurons in the first hidden layer have learned simple patterns, while the neurons in the second hidden layer have learned to combine the simple patterns of the first hidden layer into more complex patterns. In general, the more layers there are, the more complex the patterns can be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b). **Activation functions.** Try replacing the tanh activation function with a ReLU activation function, and train the network again. Notice that it finds a solution even faster, but this time the boundaries are linear. This is due to the shape of the ReLU function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c.) **The risk of local minima.** Modify the network architecture to have just one hidden layer with three neurons. Train it multiple times (to reset the network weights, click the Reset button next to the Play button). Notice that the training time varies a lot, and sometimes it even gets stuck in a local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d.) **What happens when neural nets are too small.** Remove one neuron to keep just two. Notice that the neural network is now incapable of finding a good solution, even if you try multiple times. The model has too few parameters and systematically underfits the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(e.) **What happens when neural nets are large enough.** Set the number of neurons to eight, and train the network several times. Notice that it is now consistently fast and never gets stuck. This highlights an important finding in neural network theory: large neural networks rarely get stuck in local minima, and even when they do these local optima are often almost as good as the global optimum. However, they can still get stuck on long plateaus for a long time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f.) **The risk of vanishing gradients in deep networks.** Select the spiral dataset (the bottom-right dataset under “DATA”), and change the network architecture to have four hidden layers with eight neurons each. Notice that training takes much longer and often gets stuck on plateaus for long periods of time. Also notice that the neurons in the highest layers (on the right) tend to evolve faster than the neurons in the lowest layers (on the left). This problem, called the vanishing gradients problem, can be alleviated with better weight initialization and other techniques, better optimizers (such as AdaGrad or Adam), or batch normalization (discussed in Chapter 11).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(g.) **Report your experience.** Write a few sentences about something you learned or gained a better understanding of with this problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Wildfire Classification \n",
    "\n",
    "Find the file called “utah fires.csv” (in “Fires” folder of [data repo](https://github.com/esnt/Data)). This is a subset of a Kaggle dataset about wildfires in the United States from 1992 - 2015. I’d like you to build a classifier that predicts if the fire was started naturally or by a human.\n",
    "\n",
    "\n",
    "Prepare the data:\n",
    "* Split the data into a training and test set with 25% of the data in the test set and random state=307.\n",
    "* Create a processing pipeline using that fills missing values and transforms the predictors to the same scale.\n",
    "* Update the X-training data by first fitting and then transforming the original X- training data with the processing pipeline\n",
    "* Update the X-test data by transforming the original X-test data with the pipeline trained in the previous step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a.) Build a quick and easy baseline model, for example a random forest classifier.  \n",
    "* Report and the test accuracy\n",
    "* Plot the ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b.) How many parameters are in an ANN with 2 hidden layers (100 and 50 nodes, respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c.) Build the model from (b.) with the ReLU activation function and a learning rate of 0.1 using:\n",
    "* MLPClassifer in `sklearn`\n",
    "* Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d.) For model in part (c.) (either one, they should be very similar), report the test accuracy and plot the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e.)  Compare the MLP performance to the random forest.  Is there anything that you would suggest to help improve either model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [this kaggle flower recognition dataset](https://www.kaggle.com/datasets/kausthubkannan/5-flower-types-classification-dataset/) to build a deep neural network that classifies the type of flower based on an image.  The code below will read the images into a training and validation dataset and resize the images to all have the same size.  The code assumes that the director structure is:\n",
    "```\n",
    "flower_images/\n",
    "├── Lilly/\n",
    "│   ├── image1.jpg\n",
    "│   ├── image2.jpg\n",
    "│   └── ...\n",
    "├── Lotus/\n",
    "│   ├── image1.jpg\n",
    "│   ├── image2.jpg\n",
    "│   └── ...\n",
    "├── Orchid/\n",
    "│   ├── image1.jpg\n",
    "│   ├── image2.jpg\n",
    "│   └── ...\n",
    "├── Sunflower/\n",
    "│   ├── image1.jpg\n",
    "│   ├── image2.jpg\n",
    "│   └── ...\n",
    "├── Tulip/\n",
    "│   ├── image1.jpg\n",
    "│   ├── image2.jpg\n",
    "│   └── ...\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, InputLayer\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    'flower_images/',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    'flower_images/',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practice building different types of neural networks with this data.  See the instructions below.  \n",
    "\n",
    "*If your computer doesn't have enough memory and/or processing power, you could consider using Google Colab where you have access to a GPU.  On my computer:*\n",
    "* *a FFN with two hidden layers and 10 epochs took about 2 minutes* \n",
    "* *a CNN with three convolutional layers, one hidden dense layer and 10 epochs took about 4 minutes* \n",
    "* *the pre-trained model with 10 epochs took 22 minutes*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**(a.) Build a Feed Forward Neural Network (FFN) to classify the flower images.**\n",
    "\n",
    "**Tips:**\n",
    "- Remember to flatten the images into 1D arrays (could be done with a flatten layer).\n",
    "- Experiment with different numbers of layers and nodes per layer.\n",
    "- For an FFN, start with 2-3 hidden layers and 256-512 nodes per layer.\n",
    "- Remember to use the appropriate input shape for the first layer.\n",
    "- Overfitting can be a concern, so consider regularization techniques if necessary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**(b.) Convolutional Neural Network (CNN) to classify the flower images**\n",
    "\n",
    "**Tips:**\n",
    "- Start with 2-3 convolutional layers, increasing the number of filters in each layer (e.g., 32, 64, 128).\n",
    "- A typical kernel size is `(3,3)`.\n",
    "- Use a max pooling layer after each convolutional layer\n",
    "- After convolutional layers, flatten the output and use one or two dense layers before the final classification layer.\n",
    "- Experiment with the number of filters, kernel size, and pooling size.\n",
    "- You can introduce `Dropout` layers to prevent overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**(c.) Borrow strength from a pre-trained model to classify the flower images.**\n",
    "\n",
    "**Instructions:**\n",
    "- Use a pre-trained model, such as VGG16 or ResNet50 \n",
    "- Import your pre-trained model without the top classification layer.\n",
    "- Freeze the layers of the pre-trained model to retain their learned weights.\n",
    "- Add custom layers on top of the pre-trained model.\n",
    "- Train only the custom layers (or fine-tune some top layers if desired).\n",
    "- When adding custom layers, consider starting with a `Flatten` or `GlobalAveragePooling2D` layer followed by one or two `Dense` layers.\n",
    "- Using a smaller learning rate can be beneficial when fine-tuning to avoid large weight updates.\n",
    "- If you opt to fine-tune some layers of the pre-trained model, be careful about overfitting. A few last layers are often enough.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**(d.) Evaluation:**\n",
    "\n",
    "After training each of the above models:\n",
    "\n",
    "- Plot the training and validation accuracy and loss over epochs.\n",
    "- Compare the final accuracy, precision, recall, and F1-score of the three models on the test dataset.\n",
    "- Provide insights on which model performed best and hypothesize why.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
